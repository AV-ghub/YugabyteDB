```
–∞ –º–æ–∂–µ–º –º—ã –Ω–∞–≥–µ–Ω–µ—Ä–∏—Ç—å —Ç–µ–ø–µ—Ä—å —Ç–µ—Ö –∂–µ –¥–∞–Ω–Ω—ã—Ö, –Ω–æ 100 –º–ª–Ω —Å—Ç—Ä–æ–∫?
—Ç–∞–∫ –ø–æ–Ω–∏–º–∞—é –µ—Å–ª–∏ –¥–µ–ª–∞—Ç—å —ç—Ç–æ —á–µ—Ä–µ–∑ —Å–∫—Ä–∏–ø—Ç
docker exec -it yugabyte_yb-tserver1_1 bash -c "
 echo 'Creating CSV file...'
 echo 'bucket,item_id,value,payload' > /tmp/bulk_data.csv
 for i in \$(seq 1 100000000); do
   bucket=\$((i % 1000))
   item_id=\"item_\$i\"
   # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ü–µ–ª—ã–µ —á–∏—Å–ª–∞ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è
   value=\$((RANDOM % 10000))  # 0-9999
   payload=\"payload_\$i\"
   echo \"\$bucket,\$item_id,\$value,\$payload\" >> /tmp/bulk_data.csv
 done
 echo 'CSV created with 100K rows'
 echo 'First 3 lines:'
 head -3 /tmp/big_data.csv
 "
—Ç–æ csv —Ñ–æ—Ä–º–∞—Ç —Ç–∞–∫–æ–≥–æ –Ω–µ –≤—ã–¥–µ—Ä–∂–∏—Ç
–∏–ª–∏ —ç—Ç–æ –¥–ª—è –Ω–µ–≥–æ –±—É–¥–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ?
–¥–∞–Ω–Ω—ã–µ –º—ã –ø–æ—Ç–æ–º —Å–º–æ–∂–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑
docker exec -it yugabyte_yb-tserver1_1 /home/yugabyte/bin/ycqlsh 172.26.0.4 -e "
 USE stress;
 COPY ycql_perf_test (bucket, item_id, value, payload)
 FROM '/tmp/bulk_data.csv'
 WITH HEADER = true
 AND MAXATTEMPTS = 5
 AND CHUNKSIZE = 1000
 AND INGESTRATE = 1000;
 "
?
—ç—Ç–æ —Ä–µ–∞–ª—å–Ω–æ?
```

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏ –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–µ–ª–∞—é—Ç —ç—Ç–æ –Ω–µ—Ü–µ–ª–µ—Å–æ–æ–±—Ä–∞–∑–Ω—ã–º.**

–î–∞–≤–∞–π—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º –ø–æ—á–µ–º—É –∏ –∫–∞–∫ –º–æ–∂–Ω–æ –æ–±–æ–π—Ç–∏ –ø—Ä–æ–±–ª–µ–º—É.

## üö´ –ü–æ—á–µ–º—É –≤–∞—à –ø–æ–¥—Ö–æ–¥ –ù–ï —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è 100M —Å—Ç—Ä–æ–∫:

### 1. **–†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –±—É–¥–µ—Ç –æ–≥—Ä–æ–º–Ω—ã–º**
- 100M —Å—Ç—Ä–æ–∫ √ó ~50 –±–∞–π—Ç/—Å—Ç—Ä–æ–∫–∞ = **~5 –ì–ë** CSV —Ñ–∞–π–ª
- Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –æ–±—ã—á–Ω–æ –∏–º–µ—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –¥–∏—Å–∫–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –≤ `/tmp`
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ bash —Ü–∏–∫–ª –∑–∞–π–º–µ—Ç **—á–∞—Å—ã, –µ—Å–ª–∏ –Ω–µ –¥–Ω–∏**

### 2. **–ü—Ä–æ–±–ª–µ–º—ã —Å bash-–≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π:**
- `seq 1 100000000` —Å–æ–∑–¥–∞—Å—Ç —Å–ø–∏—Å–æ–∫ –∏–∑ 100M —á–∏—Å–µ–ª –≤ –ø–∞–º—è—Ç–∏
- `RANDOM % 10000` –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–æ 32767, –ø–ª–æ—Ö–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
- –≠—Ö–æ –≤ —Ñ–∞–π–ª –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏ - –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ

### 3. **–ü—Ä–æ–±–ª–µ–º—ã —Å –∑–∞–≥—Ä—É–∑–∫–æ–π:**
- `COPY` –≤ ycqlsh –º–æ–∂–µ—Ç —É–ø–∞—Å—Ç—å –Ω–∞ —Ç–∞–π–º–∞—É—Ç–∞—Ö
- –û–¥–Ω–∞ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è –Ω–∞ –≤–µ—Å—å —Ñ–∞–π–ª - —Ä–∏—Å–∫ –ø–æ—Ç–µ—Ä–∏ –¥–∞–Ω–Ω—ã—Ö
- –ù–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª—è –Ω–∞–¥ –ø—Ä–æ—Ü–µ—Å—Å–æ–º

## ‚úÖ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–ø–æ—Å–æ–±—ã –∑–∞–≥—Ä—É–∑–∫–∏ 100M —Å—Ç—Ä–æ–∫

### **–°–ø–æ—Å–æ–± 1: –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —á–∞—Å—Ç—è–º–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)**

```bash
# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º –ø–æ 1M —Å—Ç—Ä–æ–∫ –∑–∞ —Ä–∞–∑
for batch in {1..100}; do
    echo "Processing batch $batch/100..."
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 1M —Å—Ç—Ä–æ–∫ —á–µ—Ä–µ–∑ Python (–±—ã—Å—Ç—Ä–æ)
    docker exec yugabyte_yb-tserver1_1 python3 << EOF
import random, csv, time
batch_num = $batch
start = (batch_num - 1) * 1000000 + 1
end = batch_num * 1000000

with open(f'/tmp/batch_{batch_num:03d}.csv', 'w') as f:
    writer = csv.writer(f)
    writer.writerow(['bucket', 'item_id', 'value', 'payload'])
    
    chunk_size = 100000
    for chunk_start in range(start, end + 1, chunk_size):
        chunk_end = min(chunk_start + chunk_size - 1, end)
        rows = []
        for i in range(chunk_start, chunk_end + 1):
            rows.append([i % 1000, f'item_{i}', random.randint(0, 9999), f'payload_{i}'])
        writer.writerows(rows)
        
        if chunk_start % 1000000 == 0:
            print(f"Generated {chunk_start:,} rows...")

print(f"Batch {batch_num}: Generated {end-start+1:,} rows")
EOF

    # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞
    docker exec yugabyte_yb-tserver1_1 /home/yugabyte/bin/ycqlsh 172.26.0.4 -e "
USE stress;
COPY ycql_perf_test (bucket, item_id, value, payload) 
FROM '/tmp/batch_${batch:03d}.csv' 
WITH HEADER = true 
AND CHUNKSIZE = 1000 
AND INGESTRATE = 5000;
"
    
    # –û—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–∞
    docker exec yugabyte_yb-tserver1_1 rm -f /tmp/batch_${batch:03d}.csv
    
    echo "Batch $batch loaded"
done
```

### **–°–ø–æ—Å–æ–± 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞**

```bash
# 1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ yb-bulk-load (—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è YugabyteDB)
docker exec yugabyte_yb-tserver1_1 bash -c "
wget -O /tmp/yb-bulk-load https://github.com/yugabyte/yb-bulk-load/releases/download/v1.0.0/yb-bulk-load
chmod +x /tmp/yb-bulk-load
"

# 2. –°–æ–∑–¥–∞–π—Ç–µ –∫–æ–Ω—Ñ–∏–≥
cat > bulk_load_config.json << EOF
{
  "tableName": "stress.ycql_perf_test",
  "schema": "bucket INT, item_id TEXT, value DOUBLE, payload TEXT",
  "columnMapping": ["bucket", "item_id", "value", "payload"],
  "fileFormat": "CSV",
  "csvFormat": {
    "delimiter": ",",
    "header": true
  },
  "connection": {
    "hosts": ["172.26.0.4:9042"],
    "keyspace": "stress"
  },
  "batchSize": 1000,
  "parallelism": 10
}
EOF

# 3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä + –∑–∞–≥—Ä—É–∑–∫—É –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
docker exec yugabyte_yb-tserver1_1 python3 << 'EOF'
import subprocess, random, tempfile, os
from concurrent.futures import ThreadPoolExecutor

def generate_and_load(start, end, batch_id):
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤ –ø–∞–º—è—Ç–∏ –∏ –ø—Ä—è–º–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞
    import io, csv
    from cassandra.cluster import Cluster
    
    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerow(['bucket', 'item_id', 'value', 'payload'])
    
    for i in range(start, end + 1):
        writer.writerow([i % 1000, f'item_{i}', random.random() * 10000, f'payload_{i}'])
        if (i - start + 1) % 100000 == 0:
            print(f"Batch {batch_id}: Generated {i-start+1:,} rows")
    
    output.seek(0)
    
    # –ü—Ä—è–º–∞—è –≤—Å—Ç–∞–≤–∫–∞ —á–µ—Ä–µ–∑ –¥—Ä–∞–π–≤–µ—Ä
    cluster = Cluster(['172.26.0.4'], port=9042)
    session = cluster.connect('stress')
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º batch-–≤—Å—Ç–∞–≤–∫–∏
    from cassandra.query import BatchStatement
    insert_stmt = session.prepare(
        "INSERT INTO ycql_perf_test (bucket, item_id, value, payload) VALUES (?, ?, ?, ?)"
    )
    
    reader = csv.reader(output)
    next(reader)  # Skip header
    
    batch_size = 100
    batch = BatchStatement()
    count = 0
    
    for row in reader:
        batch.add(insert_stmt, (int(row[0]), row[1], float(row[2]), row[3]))
        count += 1
        
        if count % batch_size == 0:
            session.execute(batch)
            batch = BatchStatement()
    
    if count % batch_size != 0:
        session.execute(batch)
    
    print(f"Batch {batch_id}: Loaded {count:,} rows")
    return count

# –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ 100M —Å—Ç—Ä–æ–∫
total_rows = 100_000_000
batch_size = 1_000_000
num_batches = total_rows // batch_size

with ThreadPoolExecutor(max_workers=5) as executor:
    futures = []
    for batch in range(num_batches):
        start = batch * batch_size + 1
        end = (batch + 1) * batch_size
        futures.append(executor.submit(generate_and_load, start, end, batch))
    
    total_loaded = sum(f.result() for f in futures)
    print(f"Total loaded: {total_loaded:,} rows")
EOF
```

### **–°–ø–æ—Å–æ–± 3: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Apache Spark (–¥–ª—è production)**

```python
# spark_bulk_load.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, rand, expr
import random

spark = SparkSession.builder \
    .appName("YCQL Bulk Load") \
    .config("spark.cassandra.connection.host", "172.26.0.4") \
    .config("spark.cassandra.connection.port", "9042") \
    .getOrCreate()

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 100M —Å—Ç—Ä–æ–∫ –≤ Spark
df = spark.range(1, 100000001) \
    .select(
        (col("id") % 1000).alias("bucket"),
        expr("concat('item_', id)").alias("item_id"),
        (rand() * 10000).cast("double").alias("value"),
        expr("concat('payload_', id)").alias("payload")
    )

# –ó–∞–ø–∏—Å—å –≤ YCQL
df.write \
    .format("org.apache.spark.sql.cassandra") \
    .mode("append") \
    .options(table="ycql_perf_test", keyspace="stress") \
    .save()

print("100M rows loaded!")
```

## ‚ö° –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π —Ç–µ—Å—Ç –∑–∞ —Ä–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è

**–î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –Ω–∞—á–∞—Ç—å —Å 10M —Å—Ç—Ä–æ–∫:**

```bash
# –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ 10M —Å—Ç—Ä–æ–∫
docker exec yugabyte_yb-tserver1_1 python3 << 'EOF'
from cassandra.cluster import Cluster
from cassandra.query import BatchStatement
import random, time

cluster = Cluster(['172.26.0.4'], port=9042)
session = cluster.connect('stress')

insert_stmt = session.prepare(
    "INSERT INTO ycql_perf_test (bucket, item_id, value, payload) VALUES (?, ?, ?, ?)"
)

start_time = time.time()
total_rows = 10_000_000
batch_size = 500

print(f"Starting load of {total_rows:,} rows...")

for i in range(1, total_rows + 1):
    bucket = i % 1000
    item_id = f'item_{i}'
    value = random.random() * 10000
    payload = f'payload_{i}'
    
    if i % batch_size == 1:
        batch = BatchStatement()
    
    batch.add(insert_stmt, (bucket, item_id, value, payload))
    
    if i % batch_size == 0:
        session.execute(batch)
    
    if i % 100000 == 0:
        elapsed = time.time() - start_time
        rate = i / elapsed
        print(f"Progress: {i:,} rows, Rate: {rate:,.0f} rows/sec, Elapsed: {elapsed:.1f}s")

elapsed = time.time() - start_time
print(f"\nCompleted: {total_rows:,} rows in {elapsed:.1f} seconds")
print(f"Average rate: {total_rows/elapsed:,.0f} rows/sec")
EOF
```

## üìä –û–∂–∏–¥–∞–µ–º–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:

| –û–±—ä–µ–º | –ú–µ—Ç–æ–¥ | –í—Ä–µ–º—è | –°–∫–æ—Ä–æ—Å—Ç—å | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|-------|-------|-------|----------|--------------|
| **100K** | COPY –∏–∑ CSV | 1-2 –º–∏–Ω | ~1K rows/sec | –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è |
| **10M** | Python + Batch | 10-20 –º–∏–Ω | 8-15K rows/sec | –î–ª—è –¥–µ–º–æ –Ω–∞–≥—Ä—É–∑–æ–∫ |
| **100M** | –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π Python | 1.5-3 —á–∞—Å–∞ | 10-20K rows/sec | –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç |
| **100M+** | Apache Spark | 30-60 –º–∏–Ω | 30-50K rows/sec | Production |

## üéØ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç:

**–ù–∞—á–Ω–∏—Ç–µ —Å 10M —Å—Ç—Ä–æ–∫** - —ç—Ç–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏:
1. –ü–æ–∫–∞–∂–µ—Ç —Ä–µ–∞–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å YCQL
2. –ó–∞–π–º–µ—Ç —Ä–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è (15-30 –º–∏–Ω—É—Ç)
3. –ü–æ–∑–≤–æ–ª–∏—Ç –ø—Ä–æ–≤–µ—Å—Ç–∏ –≤—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```bash
# –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç: –∑–∞–≥—Ä—É–∑–∏—Ç–µ 10M —Å—Ç—Ä–æ–∫
docker exec yugabyte_yb-tserver1_1 python3 << 'EOF'
# ... –∫–æ–¥ –¥–ª—è 10M —Å—Ç—Ä–æ–∫ –≤—ã—à–µ
EOF

# –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ–¥–∏—Ç–µ —Ç–µ—Å—Ç—ã
docker exec -it yugabyte_yb-tserver1_1 /home/yugabyte/bin/ycqlsh 172.26.0.4 -e "
USE stress;
SELECT COUNT(*) FROM ycql_perf_test;
SELECT * FROM ycql_perf_test WHERE bucket = 1 AND item_id = 'item_50000';
"
```

**–í—ã–≤–æ–¥:** 100M —Å—Ç—Ä–æ–∫ –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–∂–Ω–æ, –Ω–æ –ª—É—á—à–µ –Ω–∞—á–∞—Ç—å —Å 10M –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π. –ì–ª–∞–≤–Ω–æ–µ - –Ω–µ –æ–¥–∏–Ω –æ–≥—Ä–æ–º–Ω—ã–π —Ñ–∞–π–ª, –∞ –ø–æ—Ç–æ–∫–æ–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –±–∞—Ç—á–∞–º–∏.
