### [Replication in DocDB](https://docs.yugabyte.com/stable/architecture/docdb-replication/replication/)
[Replication factor](https://docs.yugabyte.com/stable/architecture/docdb-replication/replication/#replication-factor)
YugabyteDB replicates data across fault domains (which, depending on the deployment, could be nodes, availability zones, or regions) in order to tolerate faults.  
The replication factor **(RF) is the number of copies of data** in a YugabyteDB cluster.  

Each tablet comprises of **a set of tablet peers**, each of which stores one copy of the data belonging to the tablet.  
> **There are as many tablet peers for a tablet as the replication factor**, and they form a **Raft group**.
 
The tablet peers are **hosted on different nodes** to allow data redundancy to protect against node failures.  
**The replication** of data between the tablet peers **is strongly consistent**.

**Tablet leader replicates** among the tablet peers using Raft **to achieve strong consistency**.

The **Raft log** is used to ensure that the database **state-machine of a tablet** is replicated amongst the tablet peers with strict ordering and correctness guarantees.

**After the Raft log is replicated** to a majority of tablet-peers and successfully persisted on the majority, the **write is applied** into the DocDB document storage layer and is subsequently available for reads.   
**After the write is persisted on disk** by the document storage layer, the **write entries can be purged** from the Raft log. 

The recovery point objective (**RPO**) for each of these tablets **is 0**, meaning no data is lost in the failover to another zone.  
The recovery time objective (**RTO**) **is 3 seconds**, which is the time window for completing the failover and becoming operational out of the new zones

YugabyteDB offers **reading from followers** with relaxed guarantees.

### [Data Replication in YugabyteDB](https://www.yugabyte.com/blog/data-replication/)

YugabyteDB has developed a storage layer— **DocDB**—that is a **customized version of RocksDB**.

Yugabyte database also provides data replication at different levels in **both synchronous and asynchronous patterns**.

**Replication occurs at the tablet level** through the RAFT protocol.

#### How Synchronous Replication Works in YugabyteDB
To overcome the challenge of achieving consensus among various nodes, YugabyteDB employs the **RAFT** (Replication and Fault Tolerance) protocol.

leader-based consensus protocol facilitates **log flows from the leader to followers**.

The leader **sends the log entry** to all tablet peers and waits for acknowledgment from at least half of them.    
**Only after receiving these acknowledgments** does the leader commit the change and acknowledge the client.

#### Read Replica in YugabyteDB
**Read replica is a separate cluster** within the same universe that can serve read-only workloads.   
DocDB utilizes RAFT replication to asynchronously copy to read replica nodes (also known as observer nodes).

**All schema changes are replicated** with eventual consistency through RAFT.    
**Any config changes** and software upgrades are at the universe level; therefore, they are **automatically propagated** to the Read Replica clusters as well. 

#### xCluster Replication in YugabyteDB
In scenarios that require business continuity or disaster recovery solutions and **where complete region loss is a concern**, data must be replicated across **two independent clusters**.

YugabyteDB provides xCluster replication, **similar to the disaster recovery (DR)** solutions offered by traditional relational databases.   
xCluster replication can replicate data across independent primary clusters **asynchronously**.   
The data is replicated at the **DocDB level** with the committed data replicated to the target cluster.   
Currently, xCluster **supports both active-passive unidirectional and active-active** bidirectional replication.   

##### Non-Transactional Replication
The target universe is on the **READ COMMITTED** isolation level which only guarantees that the reader does not see any uncommitted transactions.

##### Transactional Replication
With transactional replication, the **only supported topology is active-passive** unidirectional replication.    
The target cluster maintains an xCluster Safe Time, which is the **consistent snapshot** time that the queries would read the data.

#### Change Data Capture
With YugabyteDB, the yb-tserver has a **stateless CDC service that is used to create a stream** of database changes.   
**Debezium connector** for YugabyteDB can be used to consume events generated by the CDC stream and pushed **into a Kafka topic**. 

**The order of changes delivered is guaranteed only if the changes are within a single tablet**.   
Any changes from multiple tablets can be received **out of order**.





















